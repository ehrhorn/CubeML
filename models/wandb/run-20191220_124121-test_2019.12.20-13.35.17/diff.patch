diff --git a/models/oscnext-genie-level5-v01-01-pass2/regression/vertex_reg/.gitignore b/models/oscnext-genie-level5-v01-01-pass2/regression/vertex_reg/.gitignore
index e381200..de64cbc 100644
--- a/models/oscnext-genie-level5-v01-01-pass2/regression/vertex_reg/.gitignore
+++ b/models/oscnext-genie-level5-v01-01-pass2/regression/vertex_reg/.gitignore
@@ -1,2 +1,8 @@
 /test_2019.12.20-11.04.22
 /test_2019.12.20-12.42.39
+/test_2019.12.20-13.18.30
+/test_2019.12.20-13.21.16
+/test_2019.12.20-13.21.36
+/test_2019.12.20-13.32.06
+/test_2019.12.20-13.33.44
+/test_2019.12.20-13.35.17
diff --git a/models/wandb/.gitignore b/models/wandb/.gitignore
index 9c85567..1cf9734 100644
--- a/models/wandb/.gitignore
+++ b/models/wandb/.gitignore
@@ -11,3 +11,9 @@
 /run-20191219_153838-test_2019.12.19-16.37.47
 /run-20191220_100422-test_2019.12.20-11.04.22
 /run-20191220_114239-test_2019.12.20-12.42.39
+/run-20191220_121830-test_2019.12.20-13.18.30
+/run-20191220_122116-test_2019.12.20-13.21.16
+/run-20191220_122136-test_2019.12.20-13.21.36
+/run-20191220_123206-test_2019.12.20-13.32.06
+/run-20191220_123344-test_2019.12.20-13.33.44
+/run-20191220_123517-test_2019.12.20-13.35.17
diff --git a/src/modules/classes.py b/src/modules/classes.py
index 5414023..3c012f3 100644
--- a/src/modules/classes.py
+++ b/src/modules/classes.py
@@ -264,23 +264,22 @@ class LstmPredictLoader(data.Dataset):
             self.seq_features = {}
             self.targets = {} 
 
+            # * If key does not exist, it means the key hasn't been transformed - it is therefore located raw/key
             for key in scalar_features:          
                 try:
                     self.scalar_features[key] = f[data_address+key][indices]
-                
-                # * If key does not exist, it means the key hasn't been transformed - it is therefore located raw/key
                 except KeyError:
                     self.scalar_features[key] = f['raw/'+key][indices]
 
             for key in seq_features:
-                self.seq_features[key] = f[data_address+key][indices]
-            
-            for key in targets:
+                try:
+                    self.seq_features[key] = f[data_address+key][indices]
+                except KeyError:
+                    self.seq_features[key] = f['raw/'+key][indices]
 
+            for key in targets:
                 try:
                     self.targets[key] = f[data_address+key][indices]
-                
-                # * If key does not exist, it means the key hasn't been transformed - it is therefore located raw/key
                 except KeyError:
                     self.targets[key] = f['raw/'+key][indices]
 
@@ -634,6 +633,8 @@ def load_data(hyper_pars, data_pars, architecture_pars, meta_pars, keyword):
             batch_size = data_pars['val_batch_size']
             n_events_wanted = data_pars.get('n_val_events_wanted', np.inf)
         prefix = 'transform'+str(file_keys['transform'])+'/'
+        if file_keys['transform'] == -1:
+            prefix = 'raw/'
 
         dataloader = FullBatchLoader(data_dir, seq_features, scalar_features, targets, keyword, train_frac, val_frac, test_frac, batch_size, prefix=prefix, n_events_wanted=n_events_wanted)
 
@@ -647,7 +648,7 @@ def load_data(hyper_pars, data_pars, architecture_pars, meta_pars, keyword):
     
     return dataloader
     
-def load_predictions(data_pars, keyword, file):
+def load_predictions(data_pars, meta_pars, keyword, file):
 
     cond1 = 'LstmLoader' == data_pars['dataloader']
     cond2 = 'SeqScalarTargetLoader' == data_pars['dataloader']
@@ -656,7 +657,7 @@ def load_predictions(data_pars, keyword, file):
         
         seq_features = data_pars['seq_feat'] # * feature names in sequences (if using LSTM-like network)
         scalar_features = data_pars['scalar_feat'] # * feature names
-        targets = data_pars['target'] # * target names
+        targets = get_target_keys(data_pars, meta_pars) # * target names
         train_frac = data_pars['train_frac'] # * how much data should be trained on?
         val_frac = data_pars['val_frac'] # * how much data should be used for validation?
         test_frac = data_pars['test_frac'] # * how much data should be used for training
diff --git a/src/modules/eval_funcs.py b/src/modules/eval_funcs.py
index 889b3cd..fc2233a 100644
--- a/src/modules/eval_funcs.py
+++ b/src/modules/eval_funcs.py
@@ -252,9 +252,58 @@ def vertex_x_error(pred, truth):
     x_pred = pred[x_key]
     x_true = truth[x_key]
 
-    dir_pred = torch.tensor(x_pred)
-    dir_truth = torch.tensor(x_true], dtype=dir_pred.dtype)
+    x_pred = torch.tensor(x_pred)
+    x_truth = torch.tensor(x_true, dtype=dir_pred.dtype)
 
     diff = x_pred - x_truth
     print(pred)
-    print('OMEGALUL in vertex_x_error')
\ No newline at end of file
+    print('OMEGALUL in vertex_x_error')
+    return diff
+
+def vertex_x_error(pred, truth):
+
+    # * Ensure we are dealing with the right data
+    if 'true_primary_direction_x' in pred:
+        x_key = 'true_primary_entry_position_x'
+    
+    x_pred = pred[x_key]
+    x_true = truth[x_key]
+
+    x_pred = torch.tensor(x_pred)
+    x_truth = torch.tensor(x_true, dtype=x_pred.dtype)
+
+    diff = x_pred - x_truth
+    print('OMEGALUL in vertex_x_error')
+    return diff
+
+def vertex_y_error(pred, truth):
+
+    # * Ensure we are dealing with the right data
+    if 'true_primary_direction_y' in pred:
+        y_key = 'true_primary_entry_position_y'
+    
+    y_pred = pred[y_key]
+    y_true = truth[y_key]
+
+    y_pred = torch.tensor(y_pred)
+    y_truth = torch.tensor(y_true, dtype=y_pred.dtype)
+
+    diff = y_pred - y_truth
+    print('OMEGALUL in vertex_y_error')
+    return diff
+
+def vertex_z_error(pred, truth):
+
+    # * Ensure we are dealing with the right data
+    if 'true_primary_direction_z' in pred:
+        z_key = 'true_primary_entry_position_z'
+    
+    z_pred = pred[z_key]
+    z_true = truth[z_key]
+
+    z_pred = torch.tensor(z_pred)
+    z_truth = torch.tensor(z_true, dtype=z_pred.dtype)
+
+    diff = z_pred - z_truth
+    print('OMEGALUL in vertex_z_error')
+    return diff
\ No newline at end of file
diff --git a/src/modules/helper_functions.py b/src/modules/helper_functions.py
index f1d2878..e1c9bf1 100644
--- a/src/modules/helper_functions.py
+++ b/src/modules/helper_functions.py
@@ -654,7 +654,7 @@ def get_target_keys(data_pars, meta_pars):
         if meta_pars['group'] == 'direction_reg':
             target_keys = ['true_primary_direction_x', 'true_primary_direction_y', 'true_primary_direction_z']
         elif meta_pars['group'] == 'vertex_reg':
-            target_keys = ['true_primary_entry_position_x', 'true_primary_entry_position_y', 'true_primary_entry_position_z']
+            target_keys = ['true_primary_position_x', 'true_primary_position_y', 'true_primary_position_z']
         else:
             raise ValueError('Unknown regression type (%s) encountered for dataset %s!'%(meta_pars['group'], dataset_name))
     
diff --git a/src/modules/main_funcs.py b/src/modules/main_funcs.py
index a97f884..132cd97 100644
--- a/src/modules/main_funcs.py
+++ b/src/modules/main_funcs.py
@@ -263,7 +263,7 @@ def predict(save_dir, wandb_ID = None):
                 i_str = str(i_file) if i_file > 9 else '0'+str(i_file)
                 print('%s/%d: Predicting on %s'%(i_str, N_FILES, get_path_from_root(str(file))))
                 #* Extract validation data
-                val_set = load_predictions(data_pars, 'val', file)
+                val_set = load_predictions(data_pars, meta_pars, 'val', file)
                 predictions = {key: [] for key in val_set.targets}
                 truths = {key: [] for key in val_set.targets}
 
diff --git a/src/scripts/save_exp_settings.py b/src/scripts/save_exp_settings.py
index 93cc14b..ddc7913 100644
--- a/src/scripts/save_exp_settings.py
+++ b/src/scripts/save_exp_settings.py
@@ -58,15 +58,14 @@ if __name__ == '__main__':
 
 
     data_pars = {'data_dir':     data_dir,
-                'seq_feat':    ['charge', 'dom_x', 'dom_y', 'dom_z', 'time'], 
+                'seq_feat':    ['dom_charge', 'dom_x', 'dom_y', 'dom_z', 'dom_time'], 
                 'scalar_feat': ['toi_point_on_line_x', 'toi_point_on_line_y', 'toi_point_on_line_z', 'toi_direction_x', 'toi_direction_y', 'toi_direction_z', 'toi_evalratio'],
-                'target':       ['true_neutrino_entry_position_x', 'true_neutrino_entry_position_y', 'true_neutrino_entry_position_z'],
                 'n_val_events_wanted':   100,# np.inf,
                 'n_train_events_wanted': 100,# np.inf,
                 'train_frac':  0.80,
                 'val_frac':    0.20,
                 'test_frac':   0.0,
-                'file_keys':             {'transform':   0},
+                'file_keys':             {'transform':   -1},
                 'dataloader':  'FullBatchLoader',#'LstmLoader',#'LstmLoader',
                 'collate_fn': 'PadSequence',
                 'val_batch_size':      32
@@ -75,7 +74,7 @@ if __name__ == '__main__':
 
     n_seq_feat = len(data_pars['seq_feat'])
     n_scalar_feat = len(data_pars['scalar_feat'])
-    n_target = len(data_pars['target'])
+    n_target = len(get_target_keys(data_pars, meta_pars))
 
     arch_pars =         {'non_lin':             {'func':     'LeakyReLU'},
 
