diff --git a/models/oscnext-genie-level5-v01-01-pass2/regression/vertex_reg/.gitignore b/models/oscnext-genie-level5-v01-01-pass2/regression/vertex_reg/.gitignore
index e381200..de64cbc 100644
--- a/models/oscnext-genie-level5-v01-01-pass2/regression/vertex_reg/.gitignore
+++ b/models/oscnext-genie-level5-v01-01-pass2/regression/vertex_reg/.gitignore
@@ -1,2 +1,8 @@
 /test_2019.12.20-11.04.22
 /test_2019.12.20-12.42.39
+/test_2019.12.20-13.18.30
+/test_2019.12.20-13.21.16
+/test_2019.12.20-13.21.36
+/test_2019.12.20-13.32.06
+/test_2019.12.20-13.33.44
+/test_2019.12.20-13.35.17
diff --git a/models/wandb/.gitignore b/models/wandb/.gitignore
index 9c85567..0f7883f 100644
--- a/models/wandb/.gitignore
+++ b/models/wandb/.gitignore
@@ -11,3 +11,11 @@
 /run-20191219_153838-test_2019.12.19-16.37.47
 /run-20191220_100422-test_2019.12.20-11.04.22
 /run-20191220_114239-test_2019.12.20-12.42.39
+/run-20191220_121830-test_2019.12.20-13.18.30
+/run-20191220_122116-test_2019.12.20-13.21.16
+/run-20191220_122136-test_2019.12.20-13.21.36
+/run-20191220_123206-test_2019.12.20-13.32.06
+/run-20191220_123344-test_2019.12.20-13.33.44
+/run-20191220_123517-test_2019.12.20-13.35.17
+/run-20191220_124343-test_2019.12.20-13.35.17
+/run-20191220_132314-test_2019.12.20-13.35.17
diff --git a/src/modules/classes.py b/src/modules/classes.py
index 5414023..3c012f3 100644
--- a/src/modules/classes.py
+++ b/src/modules/classes.py
@@ -264,23 +264,22 @@ class LstmPredictLoader(data.Dataset):
             self.seq_features = {}
             self.targets = {} 
 
+            # * If key does not exist, it means the key hasn't been transformed - it is therefore located raw/key
             for key in scalar_features:          
                 try:
                     self.scalar_features[key] = f[data_address+key][indices]
-                
-                # * If key does not exist, it means the key hasn't been transformed - it is therefore located raw/key
                 except KeyError:
                     self.scalar_features[key] = f['raw/'+key][indices]
 
             for key in seq_features:
-                self.seq_features[key] = f[data_address+key][indices]
-            
-            for key in targets:
+                try:
+                    self.seq_features[key] = f[data_address+key][indices]
+                except KeyError:
+                    self.seq_features[key] = f['raw/'+key][indices]
 
+            for key in targets:
                 try:
                     self.targets[key] = f[data_address+key][indices]
-                
-                # * If key does not exist, it means the key hasn't been transformed - it is therefore located raw/key
                 except KeyError:
                     self.targets[key] = f['raw/'+key][indices]
 
@@ -634,6 +633,8 @@ def load_data(hyper_pars, data_pars, architecture_pars, meta_pars, keyword):
             batch_size = data_pars['val_batch_size']
             n_events_wanted = data_pars.get('n_val_events_wanted', np.inf)
         prefix = 'transform'+str(file_keys['transform'])+'/'
+        if file_keys['transform'] == -1:
+            prefix = 'raw/'
 
         dataloader = FullBatchLoader(data_dir, seq_features, scalar_features, targets, keyword, train_frac, val_frac, test_frac, batch_size, prefix=prefix, n_events_wanted=n_events_wanted)
 
@@ -647,7 +648,7 @@ def load_data(hyper_pars, data_pars, architecture_pars, meta_pars, keyword):
     
     return dataloader
     
-def load_predictions(data_pars, keyword, file):
+def load_predictions(data_pars, meta_pars, keyword, file):
 
     cond1 = 'LstmLoader' == data_pars['dataloader']
     cond2 = 'SeqScalarTargetLoader' == data_pars['dataloader']
@@ -656,7 +657,7 @@ def load_predictions(data_pars, keyword, file):
         
         seq_features = data_pars['seq_feat'] # * feature names in sequences (if using LSTM-like network)
         scalar_features = data_pars['scalar_feat'] # * feature names
-        targets = data_pars['target'] # * target names
+        targets = get_target_keys(data_pars, meta_pars) # * target names
         train_frac = data_pars['train_frac'] # * how much data should be trained on?
         val_frac = data_pars['val_frac'] # * how much data should be used for validation?
         test_frac = data_pars['test_frac'] # * how much data should be used for training
diff --git a/src/modules/eval_funcs.py b/src/modules/eval_funcs.py
index 889b3cd..216fd9b 100644
--- a/src/modules/eval_funcs.py
+++ b/src/modules/eval_funcs.py
@@ -252,9 +252,103 @@ def vertex_x_error(pred, truth):
     x_pred = pred[x_key]
     x_true = truth[x_key]
 
-    dir_pred = torch.tensor(x_pred)
-    dir_truth = torch.tensor(x_true], dtype=dir_pred.dtype)
+    x_pred = torch.tensor(x_pred)
+    x_truth = torch.tensor(x_true, dtype=dir_pred.dtype)
 
     diff = x_pred - x_truth
     print(pred)
-    print('OMEGALUL in vertex_x_error')
\ No newline at end of file
+    print('OMEGALUL in vertex_x_error')
+    return diff
+
+def vertex_x_error(pred, truth):
+    """Calculates the error on the x-coordinate prediction of the neutrino interaction vertex.
+    
+    Arguments:
+        pred {dict} -- dictionary containing the key 'true_primary_position_x' or 'x' and the predictions.
+        truth {dict} -- dictionary containing the true values and the key 'true_primary_position_x'.   
+    
+    Raises:
+        KeyError: If wrong dictionary given
+    
+    Returns:
+        [torch.tensor] -- Signed error on prediction.
+    """    
+
+    # * Ensure we are dealing with the right data
+    if 'true_primary_position_x' in pred:
+        x_key = 'true_primary_position_x'
+    elif 'x' in pred:
+        x_key = 'x'
+    else:
+        raise KeyError('Wrong dictionary given to vertex_x_error!')
+    
+    x_pred = pred[x_key]
+    x_true = truth[x_key]
+
+    x_pred = torch.tensor(x_pred)
+    x_truth = torch.tensor(x_true, dtype=x_pred.dtype)
+
+    diff = x_pred - x_truth
+    return diff
+
+def vertex_y_error(pred, truth):
+    """Calculates the error on the y-coordinate prediction of the neutrino interaction vertex.
+    
+    Arguments:
+        pred {dict} -- dictionary containing the key 'true_primary_position_y' or 'y and the predictions.
+        truth {dict} -- dictionary containing the true values and the key 'true_primary_position_y'.   
+    
+    Raises:
+        KeyError: If wrong dictionary given
+    
+    Returns:
+        [torch.tensor] -- Signed error on prediction.
+    """    
+
+    # * Ensure we are dealing with the right data
+    if 'true_primary_position_y' in pred:
+        y_key = 'true_primary_position_y'
+    elif 'y' in pred:
+        y_key = 'y'
+    else:
+        raise KeyError('Wrong dictionary given to vertex_y_error!')
+    
+    y_pred = pred[y_key]
+    y_true = truth[y_key]
+
+    y_pred = torch.tensor(y_pred)
+    y_truth = torch.tensor(y_true, dtype=y_pred.dtype)
+
+    diff = y_pred - y_truth
+    return diff
+
+def vertex_z_error(pred, truth):
+    """Calculates the error on the z-coordinate prediction of the neutrino interaction vertex.
+    
+    Arguments:
+        pred {dict} -- dictionary containing the key 'true_primary_position_z' and the predictions.
+        truth {dict} -- dictionary containing the true values and the key 'true_primary_position_z'.   
+    
+    Raises:
+        KeyError: If wrong dictionary given
+    
+    Returns:
+        [torch.tensor] -- Signed error on prediction.
+    """    
+
+    # * Ensure we are dealing with the right data
+    if 'true_primary_position_z' in pred:
+        z_key = 'true_primary_position_z'
+    elif 'z' in pred:
+        z_key = 'z'
+    else:
+        raise KeyError('Wrong dictionary given to vertex_z_error!')
+    
+    z_pred = pred[z_key]
+    z_true = truth[z_key]
+
+    z_pred = torch.tensor(z_pred)
+    z_truth = torch.tensor(z_true, dtype=z_pred.dtype)
+
+    diff = z_pred - z_truth
+    return diff
\ No newline at end of file
diff --git a/src/modules/helper_functions.py b/src/modules/helper_functions.py
index f1d2878..e1c9bf1 100644
--- a/src/modules/helper_functions.py
+++ b/src/modules/helper_functions.py
@@ -654,7 +654,7 @@ def get_target_keys(data_pars, meta_pars):
         if meta_pars['group'] == 'direction_reg':
             target_keys = ['true_primary_direction_x', 'true_primary_direction_y', 'true_primary_direction_z']
         elif meta_pars['group'] == 'vertex_reg':
-            target_keys = ['true_primary_entry_position_x', 'true_primary_entry_position_y', 'true_primary_entry_position_z']
+            target_keys = ['true_primary_position_x', 'true_primary_position_y', 'true_primary_position_z']
         else:
             raise ValueError('Unknown regression type (%s) encountered for dataset %s!'%(meta_pars['group'], dataset_name))
     
diff --git a/src/modules/main_funcs.py b/src/modules/main_funcs.py
index a97f884..132cd97 100644
--- a/src/modules/main_funcs.py
+++ b/src/modules/main_funcs.py
@@ -263,7 +263,7 @@ def predict(save_dir, wandb_ID = None):
                 i_str = str(i_file) if i_file > 9 else '0'+str(i_file)
                 print('%s/%d: Predicting on %s'%(i_str, N_FILES, get_path_from_root(str(file))))
                 #* Extract validation data
-                val_set = load_predictions(data_pars, 'val', file)
+                val_set = load_predictions(data_pars, meta_pars, 'val', file)
                 predictions = {key: [] for key in val_set.targets}
                 truths = {key: [] for key in val_set.targets}
 
diff --git a/src/modules/reporting.py b/src/modules/reporting.py
index 3d8f425..b7ee1f4 100644
--- a/src/modules/reporting.py
+++ b/src/modules/reporting.py
@@ -186,7 +186,7 @@ class AziPolarPerformance:
         self.azi_sigmas, self.azi_errors = calc_perf2_as_fn_of_energy(energy, azi_error, self.bin_edges)
         print('Calculation finished!')
 
-        #* If an I3-reconstruction exists, get it
+        # * If an I3-reconstruction exists, get it
         if self._reco_keys:
             azi_crs = read_h5_directory(self.data_dir, self._reco_keys, prefix=self.prefix, from_frac=self.from_frac, to_frac=self.to_frac)
             true = read_h5_directory(self.data_dir, self._true_xyz, prefix=self.prefix, from_frac=self.from_frac, to_frac=self.to_frac)
@@ -204,7 +204,6 @@ class AziPolarPerformance:
             self.polar_crs_sigmas, self.polar_crs_errors = calc_perf2_as_fn_of_energy(energy, polar_crs_error, self.bin_edges)
             print('Calculation finished!')
 
-            azi_error = self.data_dict['azi_error']
             print('\nCalculating crs azimuthal performance...')
             self.azi_crs_sigmas, self.azi_crs_errors = calc_perf2_as_fn_of_energy(energy, azi_crs_error, self.bin_edges)
             print('Calculation finished!')            
@@ -397,7 +396,8 @@ class VertexPerformance:
 
     def _get_data_dict(self):
         full_pred_address = self._get_pred_path()
-        keys = get_target_keys(self.data_pars, self.meta_pars)
+        keys = self._get_keys()
+
         data_dict = read_predicted_h5_data(full_pred_address, keys)
         return data_dict
 
@@ -428,13 +428,13 @@ class VertexPerformance:
         return keys
 
     def _get_reco_keys(self):
-        dataset_name = get_dataset_name(self.data_dir)
+        dataset_name = get_dataset_name(self.data_pars['data_dir'])
 
         if dataset_name == 'MuonGun_Level2_139008':
             self._reco_keys = None
         elif dataset_name == 'oscnext-genie-level5-v01-01-pass2':
             self._reco_keys = ['retro_crs_prefit_x', 'retro_crs_prefit_y', 'retro_crs_prefit_z']
-            self._true_xyz = ['true_primary_entry_position_x', 'true_primary_entry_position_y',  'true_primary_entry_position_z']
+            self._true_xyz = ['true_primary_position_x', 'true_primary_position_y',  'true_primary_position_z']
         else:
             raise KeyError('Unknown dataset encountered (%s)'%(dataset_name))
 
@@ -444,87 +444,100 @@ class VertexPerformance:
 
         #* Transform back and extract values into list
         energy = inverse_transform(energy, get_project_root() + self.model_dir)
-        energy = [y for _, y in energy.items()]
-        energy = [x[0] for x in energy[0]]
+        energy = [y for _, y in energy.items()][0]
         self.counts, self.bin_edges = np.histogram(energy, bins=12)
         
-        x_error = self.data_dict['true_primary_entry_position_x']
+        x_error = self.data_dict['true_primary_position_x']
         print('\nCalculating x performance...')
         self.x_sigmas, self.x_errors = calc_perf2_as_fn_of_energy(energy, x_error, self.bin_edges)
         print('Calculation finished!')
 
-        y_error = self.data_dict['true_primary_entry_position_y']
+        y_error = self.data_dict['true_primary_position_y']
         print('\nCalculating y performance...')
         self.y_sigmas, self.y_errors = calc_perf2_as_fn_of_energy(energy, y_error, self.bin_edges)
         print('Calculation finished!')
 
-        z_error = self.data_dict['true_primary_entry_position_z']
+        z_error = self.data_dict['true_primary_position_z']
         print('\nCalculating x performance...')
         self.z_sigmas, self.z_errors = calc_perf2_as_fn_of_energy(energy, z_error, self.bin_edges)
         print('Calculation finished!')
 
-        #* If an I3-reconstruction exists, get it
+        # * Calculate one-number performance
+
+        # * If an I3-reconstruction exists, get it
         if self._reco_keys:
             pred_crs = read_h5_directory(self.data_pars['data_dir'], self._reco_keys, prefix=self.prefix, from_frac=self.from_frac, to_frac=self.to_frac)
             true = read_h5_directory(self.data_pars['data_dir'], self._true_xyz, prefix=self.prefix, from_frac=self.from_frac, to_frac=self.to_frac)
 
-            #* Ensure keys are proper so the angle calculations work
+            # * Ensure keys are proper so the angle calculations work
             pred_crs = inverse_transform(pred_crs, get_project_root() + self.model_dir)
             true = inverse_transform(true, get_project_root() + self.model_dir)
 
             pred_crs = convert_keys(pred_crs, [key for key in pred_crs], ['x', 'y', 'z'])
             true = convert_keys(true, [key for key in true], ['x', 'y', 'z'])
 
+            x_crs_error = vertex_x_error(pred_crs, true)
+            y_crs_error = vertex_y_error(pred_crs, true)
+            z_crs_error = vertex_z_error(pred_crs, true)
 
-            azi_crs_error = get_retro_crs_prefit_azi_error(azi_crs, true)
-            polar_crs_error = get_retro_crs_prefit_polar_error(azi_crs, true)
+            print('\nCalculating crs x performance...')
+            self.x_crs_sigmas, self.x_crs_errors = calc_perf2_as_fn_of_energy(energy, x_crs_error, self.bin_edges)
+            print('Calculation finished!')
 
-            print('\nCalculating crs polar performance...')
-            self.polar_crs_sigmas, self.polar_crs_errors = calc_perf2_as_fn_of_energy(energy, polar_crs_error, self.bin_edges)
+            print('\nCalculating crs y performance...')
+            self.y_crs_sigmas, self.y_crs_errors = calc_perf2_as_fn_of_energy(energy, y_crs_error, self.bin_edges)
             print('Calculation finished!')
 
-            azi_error = self.data_dict['azi_error']
-            print('\nCalculating crs azimuthal performance...')
-            self.azi_crs_sigmas, self.azi_crs_errors = calc_perf2_as_fn_of_energy(energy, azi_crs_error, self.bin_edges)
-            print('Calculation finished!')            
+            print('\nCalculating crs z performance...')
+            self.z_crs_sigmas, self.z_crs_errors = calc_perf2_as_fn_of_energy(energy, z_crs_error, self.bin_edges)
+            print('Calculation finished!')
 
             #* Calculate the relative improvement - e_diff/I3_error. Report decrease in error as a positive result
-            a, b = calc_relative_error(self.polar_crs_sigmas, self.polar_sigmas, self.polar_crs_errors, self.polar_errors)
-            self.polar_relative_improvements, self.polar_sigma_improvements = -a, b
-            a, b = calc_relative_error(self.azi_crs_sigmas, self.azi_sigmas, self.azi_crs_errors, self.azi_errors)
-            self.azi_relative_improvements, self.azi_sigma_improvements = -a, b
-        else:
-            self.polar_relative_improvements = None
-            self.polar_sigma_improvements = None
-            self.azi_relative_improvements = None
-            self.azi_sigma_improvements = None
+            a, b = calc_relative_error(self.x_crs_sigmas, self.x_sigmas, self.x_crs_errors, self.x_errors)
+            self.x_relative_improvements, self.x_sigma_improvements = -a, b
 
-    def get_azi_dict(self):
-        return {'edges': [self.bin_edges], 'y': [self.azi_sigmas], 'yerr': [self.azi_errors], 'xlabel': r'log(E) [E/GeV]', 'ylabel': 'Error [Deg]', 'grid': False}
+            a, b = calc_relative_error(self.y_crs_sigmas, self.y_sigmas, self.y_crs_errors, self.y_errors)
+            self.y_relative_improvements, self.y_sigma_improvements = -a, b
+
+            a, b = calc_relative_error(self.z_crs_sigmas, self.z_sigmas, self.z_crs_errors, self.z_errors)
+            self.z_relative_improvements, self.z_sigma_improvements = -a, b
+        else:
+            self.x_relative_improvements = None
+            self.x_sigma_improvements = None
+            self.y_relative_improvements = None
+            self.y_sigma_improvements = None
+            self.z_relative_improvements = None
+            self.z_sigma_improvements = None
     
     def get_energy_dict(self):
         return {'data': [self.bin_edges[:-1]], 'bins': [self.bin_edges], 'weights': [self.counts], 'histtype': ['step'], 'log': [True], 'color': ['lightgray'], 'twinx': True, 'grid': False, 'ylabel': 'Events'}
 
-    def get_polar_dict(self):
-        return {'edges': [self.bin_edges], 'y': [self.polar_sigmas], 'yerr': [self.polar_errors], 'xlabel': r'log(E) [E/GeV]', 'ylabel': 'Error [Deg]', 'grid': False}
+    def get_x_dict(self):
+        return {'edges': [self.bin_edges], 'y': [self.x_sigmas], 'yerr': [self.x_errors], 'xlabel': r'log(E) [E/GeV]', 'ylabel': 'Error [m]', 'grid': False}
+    def get_y_dict(self):
+        return {'edges': [self.bin_edges], 'y': [self.y_sigmas], 'yerr': [self.y_errors], 'xlabel': r'log(E) [E/GeV]', 'ylabel': 'Error [m]', 'grid': False}
+    def get_z_dict(self):
+        return {'edges': [self.bin_edges], 'y': [self.z_sigmas], 'yerr': [self.z_errors], 'xlabel': r'log(E) [E/GeV]', 'ylabel': 'Error [m]', 'grid': False}
 
-    def get_rel_azi_dict(self):
-        return {'edges': [self.bin_edges], 'y': [self.azi_relative_improvements], 'yerr': [self.azi_sigma_improvements], 'xlabel': r'log(E) [E/GeV]', 'ylabel': 'Rel. Imp.', 'grid': False}
+    def get_rel_x_dict(self):
+        return {'edges': [self.bin_edges], 'y': [self.x_relative_improvements], 'yerr': [self.x_sigma_improvements], 'xlabel': r'log(E) [E/GeV]', 'ylabel': 'Rel. Imp.', 'grid': False}
 
-    def get_rel_polar_dict(self):
-        return {'edges': [self.bin_edges], 'y': [self.polar_relative_improvements], 'yerr': [self.polar_sigma_improvements], 'xlabel': r'log(E) [E/GeV]', 'ylabel': 'Rel. Imp.', 'grid': False}
+    def get_rel_y_dict(self):
+        return {'edges': [self.bin_edges], 'y': [self.y_relative_improvements], 'yerr': [self.y_sigma_improvements], 'xlabel': r'log(E) [E/GeV]', 'ylabel': 'Rel. Imp.', 'grid': False}
+    
+    def get_rel_z_dict(self):
+        return {'edges': [self.bin_edges], 'y': [self.z_relative_improvements], 'yerr': [self.z_sigma_improvements], 'xlabel': r'log(E) [E/GeV]', 'ylabel': 'Rel. Imp.', 'grid': False}
 
     def save(self):
 
-        #* Save Azi first
-        perf_savepath = get_project_root()+self.model_dir+'/data/AziErrorPerformance.pickle'
-        img_address = get_project_root()+self.model_dir+'/figures/AziErrorPerformance.png'
-        d = self.get_azi_dict()
+        #* Save x first
+        img_address = get_project_root()+self.model_dir+'/figures/xVertexPerformance.png'
+        d = self.get_x_dict()
         h_fig = make_plot(d)
         
         if self._reco_keys:
             h_fig = make_plot(d, position=[0.125, 0.26, 0.775, 0.62])
-            d = self.get_rel_azi_dict()
+            d = self.get_rel_x_dict()
             d['subplot'] = True
             d['axhline'] = [0.0]
             h_fig = make_plot(d, h_figure=h_fig, position=[0.125, 0.11, 0.775, 0.15])
@@ -540,17 +553,43 @@ class VertexPerformance:
         #* Load img with PIL - this format can be logged
         if self.wandb_ID is not None:
             im = PIL.Image.open(img_address)
-            wandb.log({'AziErrorPerformance': wandb.Image(im, caption='AziErrorPerformance')}, commit = False)
+            wandb.log({'xVertexPerformance': wandb.Image(im, caption='xVertexPerformance')}, commit = False)
+
+    
+        #* Save y next
+        img_address = get_project_root()+self.model_dir+'/figures/yVertexPerformance.png'
+        d = self.get_y_dict()
+        h_fig = make_plot(d)
         
-        #* Save polar next
-        perf_savepath = get_project_root() + self.model_dir + '/data/PolarErrorPerformance.pickle'
-        img_address = get_project_root() + self.model_dir + '/figures/PolarErrorPerformance.png'
-        d = self.get_polar_dict()
+        if self._reco_keys:
+            h_fig = make_plot(d, position=[0.125, 0.26, 0.775, 0.62])
+            d = self.get_rel_y_dict()
+            d['subplot'] = True
+            d['axhline'] = [0.0]
+            h_fig = make_plot(d, h_figure=h_fig, position=[0.125, 0.11, 0.775, 0.15])
+            d_energy = self.get_energy_dict()
+            d_energy['savefig'] = img_address
+            _ = make_plot(d_energy, h_figure=h_fig, axes_index=0)
+        else:
+            h_fig = make_plot(d)
+            d_energy = self.get_energy_dict()
+            d_energy['savefig'] = img_address
+            _ = make_plot(d_energy, h_figure=h_fig, axes_index=0)
+
+        #* Load img with PIL - this format can be logged
+        if self.wandb_ID is not None:
+            im = PIL.Image.open(img_address)
+            wandb.log({'yVertexPerformance': wandb.Image(im, caption='yVertexPerformance')}, commit = False)
+        
+
+        #* Save z last
+        img_address = get_project_root()+self.model_dir+'/figures/zVertexPerformance.png'
+        d = self.get_z_dict()
         h_fig = make_plot(d)
         
         if self._reco_keys:
             h_fig = make_plot(d, position=[0.125, 0.26, 0.775, 0.62])
-            d = self.get_rel_polar_dict()
+            d = self.get_rel_z_dict()
             d['subplot'] = True
             d['axhline'] = [0.0]
             h_fig = make_plot(d, h_figure=h_fig, position=[0.125, 0.11, 0.775, 0.15])
@@ -566,9 +605,9 @@ class VertexPerformance:
         #* Load img with PIL - this format can be logged
         if self.wandb_ID is not None:
             im = PIL.Image.open(img_address)
-            wandb.log({'PolarErrorPerformance': wandb.Image(im, caption='PolarErrorPerformance')}, commit = False)
+            wandb.log({'zVertexPerformance': wandb.Image(im, caption='zVertexPerformance')}, commit = False)
 
-        perf_savepath = get_project_root() + self.model_dir + '/data/AziPolarPerformance.pickle'
+        perf_savepath = get_project_root() + self.model_dir + '/data/VertexPerformance.pickle'
         with open(perf_savepath, 'wb') as f:
             pickle.dump(self, f)
 
@@ -682,7 +721,8 @@ def log_performance_plots(model_dir, wandb_ID=None):
         perf.save()
 
     elif meta_pars['group'] == 'vertex_reg':
-        print('lol')
+        vertex_perf = VertexPerformance(model_dir, wandb_ID=wandb_ID)
+        vertex_perf.save()
     else:
         print('Unknown regression type - no plots have been produced.')
     print(strftime("%d/%m %H:%M", localtime()), ': Logging finished!')
diff --git a/src/scripts/save_exp_settings.py b/src/scripts/save_exp_settings.py
index 93cc14b..ddc7913 100644
--- a/src/scripts/save_exp_settings.py
+++ b/src/scripts/save_exp_settings.py
@@ -58,15 +58,14 @@ if __name__ == '__main__':
 
 
     data_pars = {'data_dir':     data_dir,
-                'seq_feat':    ['charge', 'dom_x', 'dom_y', 'dom_z', 'time'], 
+                'seq_feat':    ['dom_charge', 'dom_x', 'dom_y', 'dom_z', 'dom_time'], 
                 'scalar_feat': ['toi_point_on_line_x', 'toi_point_on_line_y', 'toi_point_on_line_z', 'toi_direction_x', 'toi_direction_y', 'toi_direction_z', 'toi_evalratio'],
-                'target':       ['true_neutrino_entry_position_x', 'true_neutrino_entry_position_y', 'true_neutrino_entry_position_z'],
                 'n_val_events_wanted':   100,# np.inf,
                 'n_train_events_wanted': 100,# np.inf,
                 'train_frac':  0.80,
                 'val_frac':    0.20,
                 'test_frac':   0.0,
-                'file_keys':             {'transform':   0},
+                'file_keys':             {'transform':   -1},
                 'dataloader':  'FullBatchLoader',#'LstmLoader',#'LstmLoader',
                 'collate_fn': 'PadSequence',
                 'val_batch_size':      32
@@ -75,7 +74,7 @@ if __name__ == '__main__':
 
     n_seq_feat = len(data_pars['seq_feat'])
     n_scalar_feat = len(data_pars['scalar_feat'])
-    n_target = len(data_pars['target'])
+    n_target = len(get_target_keys(data_pars, meta_pars))
 
     arch_pars =         {'non_lin':             {'func':     'LeakyReLU'},
 
